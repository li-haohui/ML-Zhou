{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e10c53",
   "metadata": {},
   "source": [
    "# BP 算法\n",
    "\n",
    "## 前置知识\n",
    "\n",
    "![jupyter](../../pic4jupyter/BP_NN.png)\n",
    "\n",
    "转化为数学语言既是\n",
    "$$\\begin{aligned}\n",
    "X = a^{(0)} &\\Rightarrow Z^{(1)} = W^{(1)}a^{(0)} + b^{(1)} \\\\\n",
    "            &\\Rightarrow a^{(1)} = f(Z^{(1)}) \\\\\n",
    "            &...  \\\\\n",
    "            &\\Rightarrow Z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)} \\\\\n",
    "            &\\Rightarrow y = f(Z^{(l)})\n",
    "\\end{aligned}$$\n",
    "其中若 $X$ 为 $N$ 维向量，隐层有 $M$ 个神经元，则 $W^{(1)}$ 为 $N \\times M$ 维矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e45f6",
   "metadata": {},
   "source": [
    "## 标准BP-后向传播\n",
    "\n",
    "### 计算总误差\n",
    "$$\n",
    "E_{total} = \\frac{1}{2} \\sum_{j=1}{(\\hat{y_j} - y_j)^2}\n",
    "$$\n",
    "\n",
    "### 隐层 => 输出层\n",
    "采用**梯度下降**策略\n",
    "\n",
    "$$\n",
    "\\Delta w^{(l)}_{hj} = -\\eta \\frac{\\partial E_k} {\\partial w^{(l)}_{hj}}\n",
    "$$\n",
    "根据链式法则有\n",
    "$$\n",
    "\\frac{\\partial E_k}{\\partial w^{(l)}_{hj}} = \\frac{\\partial E_k}{\\partial{\\hat{y_j}}} \\cdot \\frac{\\partial{\\hat{y_j}}}{\\partial Z^{(l)}_j} \\cdot \\frac{\\partial Z^{(l)}_j}{\\partial w^{(l)}_{hj}}\n",
    "$$\n",
    "同时又有\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial E_k}{\\partial{\\hat{y_j}}} &= \\hat{y_j} - y_j \\\\\n",
    "\\frac{\\partial{\\hat{y_j}}}{\\partial Z^{(l)}_j} &= f'(Z^{(l)}_j) \\\\\n",
    "\\frac{\\partial Z^{(l)}_j}{\\partial w^{(l)}_{hj}} &= a^{(l-1)}_h\n",
    "\\end{aligned}$$\n",
    "记\n",
    "$$\\begin{aligned}\n",
    "g^{(l)}_j &= - \\frac{\\partial E_k}{\\partial{\\hat{y_j}}} \\cdot \\frac{\\partial{\\hat{y_j}}}{\\partial Z^{(l)}_j} \\\\\n",
    "    &= - (\\hat{y_j} - y_j) \\cdot f'(Z^{(l)}_j) \\\\\n",
    "    &= - (\\hat{y_j} - y_j) \\cdot f'(W^{(l)}_h a^{(l-1)} + b^{(l)})\n",
    "\\end{aligned}$$\n",
    "故最后有\n",
    "$$\n",
    "\\Delta w^{(l)}_{hj} = \\eta g^{(l)}_j a^{(l-1)}_h\n",
    "$$\n",
    "同理可得\n",
    "$$\n",
    "\\Delta b^{(l)}_j = \\eta g^{(l)}_j\n",
    "$$\n",
    "\n",
    "### 隐层 => 隐层\n",
    "$$\n",
    "\\Delta w^{(l-1)}_{ih} = \\eta g^{(l-1)}_h a^{(l-2)} \n",
    "$$\n",
    "而这里的\n",
    "$$\\begin{aligned}\n",
    "g^{(l-1)}_h &= f'(Z^{(l-1)}) \\cdot \\sum^{l}_{j=1} w^{(l)}_{hj}g^{(l)}_j \\\\\n",
    "     &= a^{(l-1)}_h (1 - a^{(l-1)}_h) \\sum^{l}_{j=1} w^{(l)}_{hj}g^{(l)}_j\n",
    "\\end{aligned}$$\n",
    "同理有\n",
    "$$\n",
    "\\Delta b^{(l-1)}_h = \\eta g^{(l-1)}_h\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f6e571",
   "metadata": {},
   "source": [
    "相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db18f46b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbaf51",
   "metadata": {},
   "source": [
    "数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7963be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载随机数据集，定义数据集类\n",
    "class my_dataset():\n",
    "    def __init__(self):\n",
    "        self.x_train = 0\n",
    "        self.y_train = 0\n",
    "        self.x_test = 0\n",
    "        self.y_test = 0\n",
    "\n",
    "    def generate_data(self, seed = 272):\n",
    "        np.random.seed(seed)\n",
    "        # 随机生成两个均值和方差不同的数据集\n",
    "        data_size_1 = 300\n",
    "        x1_1 = np.random.normal(loc=5.0, scale=1.0, size=data_size_1)\n",
    "        x2_1 = np.random.normal(loc=4.0, scale=1.0, size=data_size_1)\n",
    "        y_1 = [0 for _ in range(data_size_1)]\n",
    "        data_size_2 = 400\n",
    "        x1_2 = np.random.normal(loc=10.0, scale=2.0, size=data_size_2)\n",
    "        x2_2 = np.random.normal(loc=8.0, scale=2.0, size=data_size_2)\n",
    "        y_2 = [1 for _ in range(data_size_2)]\n",
    "        x1 = np.concatenate((x1_1, x1_2), axis=0)\n",
    "        x2 = np.concatenate((x2_1, x2_2), axis=0)\n",
    "        x = np.hstack((x1.reshape(-1, 1), x2.reshape(-1, 1)))\n",
    "        y = np.concatenate((y_1, y_2), axis=0)\n",
    "        data_size_all = data_size_1 + data_size_2\n",
    "        shuffled_index = np.random.permutation(data_size_all) # 将数据集打乱\n",
    "        x = x[shuffled_index]\n",
    "        y = y[shuffled_index]\n",
    "        return x, y\n",
    "\n",
    "    def train_test_split(self, x, y):\n",
    "        split_index = int(len(y) * 0.7)\n",
    "        self.x_train = x[:split_index]\n",
    "        self.y_train = y[:split_index]\n",
    "        self.x_test = x[split_index:]\n",
    "        self.y_test = y[split_index:]\n",
    "\n",
    "        return self.x_train, self.y_train, self.x_test, self.y_test\n",
    "\n",
    "    def data(self):\n",
    "        x, y = self.generate_data(seed=272)\n",
    "        return self.train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f03a3",
   "metadata": {},
   "source": [
    "标准 BP_NN 类的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65f1a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准BP-NN\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, lr=0.1):\n",
    "        # 初始化权重矩阵、层数、学习率\n",
    "        # 例如：layers=[2, 3, 2]，表示输入层两个结点，隐藏层3个结点，输出层2个结点\n",
    "        self.W = []     # 权值矩阵\n",
    "        self.b = []     # 偏置矩阵\n",
    "        self.layers = layers    # 网络层数\n",
    "        self.lr = lr            # 学习率\n",
    "\n",
    "        # 随机初始化权重矩阵，如果三层网络，则有两个权重矩阵；\n",
    "        \n",
    "        for i in np.arange(0, len(layers) - 1):\n",
    "            w = np.random.rand(layers[i], layers[i+1])\n",
    "            self.W.append(w)\n",
    "            self.b.append(np.zeros((1, layers[i+1])))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # 输出网络结构\n",
    "        return \"NeuralNetwork: {}\".format(\n",
    "            \"-\".join(str(l) for l in self.layers)\n",
    "        )\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # sigmoid激活函数\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        # sigmoid的导数\n",
    "        # return np.exp(x) / (np.exp(x) + 1)**2\n",
    "        # return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "        return x * (1-x)\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, display=100):\n",
    "        # 训练网络\n",
    "        \n",
    "        # 迭代的epoch\n",
    "        loss_out = []\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # 对数据集中每一个样本执行前向传播、反向传播、更新权重\n",
    "            for (x_k, y_k) in zip(X, y):\n",
    "                self.fit_partial(x_k, y_k)\n",
    "\n",
    "            # 打印输出\n",
    "            loss, y_pred = self.calculate_loss(X, y)\n",
    "            acc = self.score(y, y_pred)\n",
    "            loss_out.append(loss)\n",
    "            if epoch == 0 or (epoch + 1) % display == 0:\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}, accuracy={:.7f}\".format(\n",
    "                    epoch + 1, loss, acc\n",
    "                ))\n",
    "        return loss_out\n",
    "    \n",
    "    def fit_partial(self, x, y):\n",
    "        # 构造一个列表A，用于保存网络的每一层的输出，即经过激活函数的输出\n",
    "        A = [np.atleast_2d(x)]\n",
    "        # ---------- 前向传播 ----------\n",
    "        # 对网络的每一层进行循环\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # 计算当前层的输出\n",
    "            z = A[-1].dot(self.W[layer]) + self.b[layer]\n",
    "            a = self.sigmoid(z)\n",
    "            # 添加到列表A\n",
    "            A.append(a)\n",
    "\n",
    "        # ---------- 反向传播 ----------\n",
    "        # 求取 gj\n",
    "        gj = [- (A[-1]-y) * self.sigmoid_deriv(A[-1])]\n",
    "\n",
    "        # 计算前面的权重矩阵的\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # 参见上文推导的公式\n",
    "            gj_ = gj[-1].dot(self.W[layer].T)\n",
    "            gj.append(gj_ * self.sigmoid_deriv(A[layer]))\n",
    "\n",
    "        # 列表gj是从后往前记录，下面更新权重矩阵的时候，是从输入层到输出层. 因此，在这里逆序\n",
    "        gj = gj[::-1]\n",
    "        # 迭代更新权重\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # 参考上文公式\n",
    "            self.W[layer] += self.lr * A[layer].T.dot(gj[layer])\n",
    "            self.b[layer] += self.lr * gj[layer]\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 预测\n",
    "        a = np.atleast_2d(X)\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            a = self.sigmoid(a.dot(self.W[layer]) + self.b[layer]) # 根据训练好的权值和偏置前向计算一次输出\n",
    "        # sigmoid输出转换成0-1标签\n",
    "        pre_lable = np.zeros(a.shape)                               \n",
    "        pre_lable[a >= 0.5] = 1\n",
    "\n",
    "        return a, pre_lable\n",
    "\n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions, pre_label = self.predict(X)\n",
    "        loss = 0.5 * np.sum((predictions.T - targets) ** 2)\n",
    "        return loss, pre_label\n",
    "\n",
    "    def score(self, y_true=None, y_pred=None):\n",
    "        if y_true is None or y_pred is None:\n",
    "            y_true = self.y\n",
    "            y_pred = self.predict()\n",
    "        acc = np.mean([1 if y_true[i] == y_pred[i] else 0 for i in range(len(y_true))])\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97c75a",
   "metadata": {},
   "source": [
    "### 累积 BP-后向传播\n",
    "\n",
    "参数更新方式同标准bp，但更新时机不同，累积 BP 在遍历一次训练集后对数据集进行更新。  \n",
    "同时最小化目标函数为训练集上的累积误差\n",
    "$$\n",
    "E = \\frac{1}{m} \\sum^{m}_{k=1}{E_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26ab2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 累积BP-NN\n",
    "class accumulate_NeuralNetwork(object):\n",
    "    def __init__(self, layers, lr=0.01):\n",
    "        # 初始化权重矩阵、层数、学习率\n",
    "        # 例如：layers=[2, 3, 2]，表示输入层两个结点，隐藏层3个结点，输出层2个结点\n",
    "        self.W = []     # 权值矩阵\n",
    "        self.b = []     # 偏置矩阵\n",
    "        self.layers = layers    # 网络层数\n",
    "        self.lr = lr            # 学习率\n",
    "\n",
    "        # 随机初始化权重矩阵，如果三层网络，则有两个权重矩阵；\n",
    "        for i in np.arange(0, len(layers) - 1):\n",
    "            w = np.random.rand(layers[i], layers[i+1])\n",
    "            self.W.append(w)\n",
    "            self.b.append(np.zeros((1, layers[i+1])))\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        # 输出网络结构\n",
    "        return \"NeuralNetwork: {}\".format(\n",
    "            \"-\".join(str(l) for l in self.layers)\n",
    "        )\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # sigmoid激活函数\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        # sigmoid的导数\n",
    "        # return np.exp(x) / (np.exp(x) + 1)**2\n",
    "        # return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "        return x * (1-x)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, display=100):\n",
    "        # 训练网络\n",
    "        # # 对训练数据添加一维值为1的特征，用于同时训练偏置的权重\n",
    "\n",
    "        # 迭代的epoch\n",
    "        loss_out = []\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # 对数据集中每一个样本执行前向传播、反向传播、更新权重\n",
    "            self.fit_partial(X, y)\n",
    "\n",
    "            # 打印输出\n",
    "            loss, y_pred = self.calculate_loss(X, y)\n",
    "            acc = self.score(y, y_pred)\n",
    "            loss_out.append(loss)\n",
    "            if epoch == 0 or (epoch + 1) % display == 0:\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}, accuracy={:.7f}\".format(\n",
    "                    epoch + 1, loss, acc\n",
    "                ))\n",
    "        return loss_out\n",
    "\n",
    "    def fit_partial(self, x, y):\n",
    "        # 构造一个列表A，用于保存网络的每一层的输出，即经过激活函数的输出\n",
    "        A = [np.atleast_2d(x)]\n",
    "        y = np.atleast_2d(y).T\n",
    "        # ---------- 前向传播 ----------\n",
    "        # 对网络的每一层进行循环\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # 计算当前层的输出\n",
    "            z = A[-1].dot(self.W[layer]) + self.b[layer]\n",
    "            a = self.sigmoid(z)\n",
    "            # 添加到列表A\n",
    "            A.append(a)\n",
    "        # ---------- 反向传播 ----------\n",
    "        # 求取 gj\n",
    "        gj = [- (A[-1]-y) * self.sigmoid_deriv(A[-1])]\n",
    "\n",
    "        # 计算前面的权重矩阵的\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # 参见上文推导的公式\n",
    "            gj_ = gj[-1].dot(self.W[layer].T)\n",
    "            gj.append(gj_ * self.sigmoid_deriv(A[layer]))\n",
    "\n",
    "        # 列表gj是从后往前记录，下面更新权重矩阵的时候，是从输入层到输出层. 因此，在这里逆序\n",
    "        gj = gj[::-1]\n",
    "        # 迭代更新权重\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # 参考上文公式\n",
    "            self.W[layer] += self.lr * A[layer].T.dot(gj[layer])\n",
    "            self.b[layer] += self.lr * np.atleast_2d(gj[layer].mean(axis=0)) # 取平均\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 预测\n",
    "        a = np.atleast_2d(X)\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            a = self.sigmoid(a.dot(self.W[layer]) + self.b[layer]) # 根据训练好的权值和偏置前向计算一次输出\n",
    "        pre_lable = np.zeros(a.shape)                               # sigmoid输出转换成0-1标签\n",
    "        pre_lable[a >= 0.5] = 1\n",
    "\n",
    "        return a, pre_lable\n",
    "\n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions, pre_label = self.predict(X)\n",
    "        Ek = 0.5 * (predictions.T - targets)**2\n",
    "        loss = np.sum(Ek) / X.shape[0]\n",
    "        # loss = np.sum(abs(predictions.T - targets)) / X.shape[0]\n",
    "\n",
    "        return loss, pre_label\n",
    "\n",
    "    def score(self, y_true=None, y_pred=None):\n",
    "        if y_true is None or y_pred is None:\n",
    "            y_true = self.y\n",
    "            y_pred = self.predict()\n",
    "        acc = np.mean([1 if y_true[i] == y_pred[i] else 0 for i in range(len(y_true))])\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1cbbe",
   "metadata": {},
   "source": [
    "构造数据并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f74b36bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork: 2-3-1\n",
      "[INFO] epoch=1, loss=53.6278652, accuracy=0.5828221\n",
      "[INFO] epoch=100, loss=3.6023261, accuracy=0.9795501\n",
      "[INFO] epoch=200, loss=3.3608705, accuracy=0.9775051\n",
      "[INFO] epoch=300, loss=3.2982341, accuracy=0.9795501\n",
      "[INFO] epoch=400, loss=3.2726094, accuracy=0.9795501\n",
      "[INFO] epoch=500, loss=3.2592984, accuracy=0.9815951\n",
      "[INFO] epoch=600, loss=3.2510558, accuracy=0.9815951\n",
      "[INFO] epoch=700, loss=3.2451814, accuracy=0.9815951\n",
      "[INFO] epoch=800, loss=3.2405275, accuracy=0.9815951\n",
      "[INFO] epoch=900, loss=3.2365707, accuracy=0.9815951\n",
      "[INFO] epoch=1000, loss=3.2330642, accuracy=0.9815951\n",
      "NeuralNetwork: 2-3-1\n",
      "[INFO] epoch=1, loss=0.1280359, accuracy=0.5828221\n",
      "[INFO] epoch=100, loss=0.1030376, accuracy=0.5828221\n",
      "[INFO] epoch=200, loss=0.0462691, accuracy=0.9775051\n",
      "[INFO] epoch=300, loss=0.0296973, accuracy=0.9856851\n",
      "[INFO] epoch=400, loss=0.0229284, accuracy=0.9815951\n",
      "[INFO] epoch=500, loss=0.0192438, accuracy=0.9795501\n",
      "[INFO] epoch=600, loss=0.0169049, accuracy=0.9775051\n",
      "[INFO] epoch=700, loss=0.0152840, accuracy=0.9775051\n",
      "[INFO] epoch=800, loss=0.0140935, accuracy=0.9775051\n",
      "[INFO] epoch=900, loss=0.0131813, accuracy=0.9775051\n",
      "[INFO] epoch=1000, loss=0.0124593, accuracy=0.9775051\n",
      "standard BP：  0.981042654028436 \n",
      " accumulate BP：  0.9620853080568721\n",
      "standard BP： \n",
      " [[ 93   3]\n",
      " [  1 114]] \n",
      " accumulate BP： \n",
      " [[ 89   7]\n",
      " [  1 114]]\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "dataset = my_dataset()\n",
    "x_train, y_train, x_test, y_test = dataset.data()\n",
    "\n",
    "# 归一化\n",
    "x_train = (x_train - np.min(x_train, axis=0)) / (np.max(x_train, axis=0) - np.min(x_train, axis=0))\n",
    "x_test = (x_test - np.min(x_test, axis=0)) / (np.max(x_test, axis=0) - np.min(x_test, axis=0))\n",
    "\n",
    "# 实例化标准BP\n",
    "nn = NeuralNetwork([x_train.shape[1], 3, 1],lr=0.1)\n",
    "print(nn.__repr__())\n",
    "\n",
    "loss = nn.fit(x_train, y_train)\n",
    "pro, y_test_pred = nn.predict(x_test)\n",
    "\n",
    "# 实例化累计BP\n",
    "acc_nn = accumulate_NeuralNetwork([x_train.shape[1], 3, 1],lr=0.01)\n",
    "print(acc_nn.__repr__())\n",
    "\n",
    "acc_loss = acc_nn.fit(x_train, y_train)\n",
    "acc_pro, acc_y_test_pred = acc_nn.predict(x_test)\n",
    "\n",
    "# 计算预测精度\n",
    "acc = nn.score(y_test_pred, y_test)\n",
    "acc_acc = acc_nn.score(acc_y_test_pred, y_test)\n",
    "print(\"standard BP： \", acc, \"\\n\", \"accumulate BP： \", acc_acc)\n",
    "\n",
    "# 输出混淆矩阵\n",
    "maxtrix = confusion_matrix(y_test, y_test_pred)\n",
    "acc_maxtrix = confusion_matrix(y_test, acc_y_test_pred)\n",
    "print(\"standard BP： \\n\", maxtrix, \"\\n\", \"accumulate BP： \\n\", acc_maxtrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425c7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyt]",
   "language": "python",
   "name": "conda-env-pyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
